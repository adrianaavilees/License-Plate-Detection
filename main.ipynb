{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67260f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from pathlib import Path\n",
    "# !pip install ultralytics\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b52be20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only characters present in Spanish license plates (no vowels)\n",
    "CHAR_CLASSES = \"0123456789BCDFGHJKLMNPQRSTVWXYZ\"\n",
    "\n",
    "class LicensePlateDetector:\n",
    "    def __init__(self):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = None\n",
    "\n",
    "    def create_dataset(self, dataset_path, yaml_path=\"dataset.yaml\"):\n",
    "        \"\"\"Create a file dataset.yaml for training the model\"\"\"\n",
    "        train_path = os.path.join(dataset_path, 'images', 'train')\n",
    "        val_path = os.path.join(dataset_path, 'images', 'val')\n",
    "        test_path = os.path.join(dataset_path, 'images', 'test')\n",
    "\n",
    "        config_content = f\"\"\"\n",
    "path: {dataset_path}\n",
    "train: {train_path}\n",
    "val: {val_path}\n",
    "test: {test_path}\n",
    "names:\n",
    "  0: license_plate\"\"\"\n",
    "        with open(yaml_path, 'w') as file:\n",
    "            file.write(config_content)\n",
    "        print(f\"Dataset configuration file created at {yaml_path}\")\n",
    "        return yaml_path\n",
    "\n",
    "    def train_model(self, dataset_path, epochs=10, batch=8, project=\"license_plate_training\"):\n",
    "        \"\"\"Train a custom model for license plate detection.\"\"\"\n",
    "        \n",
    "        yaml_path = self.create_dataset(dataset_path)\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "\n",
    "        # Load base model\n",
    "        self.model = YOLO(\"yolov8n.pt\")\n",
    "        # Train the model on the custom dataset\n",
    "        results = self.model.train(data=yaml_path, epochs=epochs, batch=batch, project=project)\n",
    "        best_model_path = Path(results.save_dir) / \"weights\" / \"best.pt\"\n",
    "        \n",
    "        print(\"Training completed\")\n",
    "        print(f\"Best model saved at: {str(best_model_path)}\")\n",
    "        return str(best_model_path)\n",
    "\n",
    "    def detect_license_plate(self, best_model_path, test_dir, output_dir, conf=0.25):\n",
    "        \"\"\"Detect license plates in images using the trained model.\"\"\"\n",
    "        self.model = YOLO(best_model_path)\n",
    "        print(f\"Detecting license plates with model {best_model_path}\")\n",
    "        \n",
    "        prediction_results = self.model.predict(source=test_dir, save=False, conf=conf)\n",
    "        \n",
    "        results_dir = os.path.join(output_dir, \"results\")\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        for i, result in enumerate(prediction_results):\n",
    "            annotated_image = result.plot()\n",
    "            output_path = os.path.join(results_dir, f\"detected_{i}.jpg\")\n",
    "            cv2.imwrite(output_path, annotated_image)\n",
    "        \n",
    "        print(\"Detection completed. Results saved.\")\n",
    "        return prediction_results\n",
    "\n",
    "    # =========================================================================\n",
    "    # Character Segmentation Functions \n",
    "    # =========================================================================\n",
    "        \n",
    "    def preprocess_plate_for_segmentation(self, plate_image):\n",
    "        \"\"\"\n",
    "        Preprocesses a license plate image for character segmentation.\n",
    "        - Transforms to grayscale and uses HSV to handle the blue section.\n",
    "        - Applies Otsu's thresholding for binarization.\n",
    "        \"\"\"\n",
    "        # Convert to HSV to better handle the blue band\n",
    "        hsv = cv2.cvtColor(plate_image, cv2.COLOR_BGR2HSV)\n",
    "        image_h, image_s, image_v = cv2.split(hsv)\n",
    "        \n",
    "        # Define range for blue color and create a mask\n",
    "        lower_blue = np.array([100, 150, 0])\n",
    "        upper_blue = np.array([140, 255, 255])\n",
    "        mask_blue = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "        \n",
    "        # Convert to grayscale\n",
    "        # gray = cv2.cvtColor(plate_image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Paint the blue area white on the grayscale image\n",
    "        image_v[mask_blue > 0] = 255\n",
    "\n",
    "        # Apply Gaussian blur to reduce the noise\n",
    "        image_v = cv2.GaussianBlur(image_v, (5, 5), 0)\n",
    "        \n",
    "        # Apply Otsu's thresholding to get a binary image\n",
    "        _, otsu_thresh = cv2.threshold(image_v, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "        \n",
    "        # Invert the image so characters are white on a black background\n",
    "        binary = cv2.bitwise_not(otsu_thresh)\n",
    "\n",
    "        # Morphological operations to clean up\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "        # CLOSING: fill small holes in the characters\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel, iterations=3)\n",
    "        #OPENING: remove small noise, and separate connected characters\n",
    "        binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "            \n",
    "        return binary\n",
    "\n",
    "    def segment_characters(self, plate_image):\n",
    "        \"\"\"\n",
    "        Finds and segments individual characters from a preprocessed license plate image.\n",
    "        - Uses cv.findContours to identify character shapes.\n",
    "        - Filters contours based on size and aspect ratio to isolate characters.\n",
    "        \"\"\"\n",
    "        preprocessed_plate = self.preprocess_plate_for_segmentation(plate_image)        \n",
    "        contours, _ = cv2.findContours(preprocessed_plate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        characters = []\n",
    "        # Get shape from preprocessed image which is grayscale (2 dimensions)\n",
    "        plate_height, plate_width = preprocessed_plate.shape\n",
    "        \n",
    "        for contour in contours:\n",
    "            x, y, w, h = cv2.boundingRect(contour)\n",
    "            aspect_ratio = w / h\n",
    "            area = cv2.contourArea(contour)\n",
    "\n",
    "            # Filter the countours that touch the border of the plate\n",
    "            # Characters should not be at the edge of the plate\n",
    "            touches_border = (x==0 or y==0 or (x+w) >= plate_width or (y+h) >= plate_height)\n",
    "            \n",
    "            # Heuristic filters to identify a character based on area and aspect ratio.\n",
    "            # We have removed the position filter to detect the first character.\n",
    "            if area > 50 and 0.1 < aspect_ratio < 1.2 and h > 10 and w > 3 and not touches_border: #! Aspect ratio expanded to include narrow character as 1\n",
    "                character_roi = plate_image[y:y+h, x:x+w]\n",
    "                characters.append({'image': character_roi, 'bbox': (x, y, w, h)})\n",
    "        \n",
    "        characters.sort(key=lambda c: c['bbox'][0])\n",
    "\n",
    "        # Maximum of 7 characters (spanish license plates)\n",
    "        if len(characters) > 7:\n",
    "            characters = sorted(characters, key=lambda c: c['bbox'][2]*c['bbox'][3], reverse=True)[:7]\n",
    "            characters.sort(key=lambda c: c['bbox'][0])\n",
    "\n",
    "        return characters, preprocessed_plate\n",
    "\n",
    "    def process_and_show_results(self, prediction_results):\n",
    "        \"\"\"Processes each detected plate to segment and show the characters.\"\"\"\n",
    "        for i, result in enumerate(prediction_results):\n",
    "            image = result.orig_img\n",
    "            boxes = result.boxes\n",
    "            \n",
    "            if boxes:\n",
    "                print(f\"\\n--- Processing Image {i+1} ---\")\n",
    "                \n",
    "                for j, box in enumerate(boxes):\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())\n",
    "                    \n",
    "                    plate_image = image[y1:y2, x1:x2]\n",
    "                    \n",
    "                    if plate_image is not None and plate_image.size > 0:\n",
    "                        print(f\"Detected License Plate {j+1}: Bbox {x1, y1, x2, y2}\")\n",
    "                        \n",
    "                        characters, preprocessed_plate = self.segment_characters(plate_image)\n",
    "                        print(f\"Found {len(characters)} character candidates.\")\n",
    "\n",
    "                        plate_text = self.ocr_characters(characters)\n",
    "                        print(f\"OCR Result, Detected Plate: {plate_text}\")\n",
    "            \n",
    "                        # Create output image with bounding boxes\n",
    "                        output_image = plate_image.copy()\n",
    "                        for char_info in characters:\n",
    "                            x, y, w, h = char_info['bbox']\n",
    "                            cv2.rectangle(output_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                        \n",
    "                        # Show images\n",
    "                        annotated_full_image = result.plot()\n",
    "                        \n",
    "                        plt.figure(figsize=(20, 5))\n",
    "                        \n",
    "                        # 1. Full image with detected plate\n",
    "                        plt.subplot(1, 4, 1)\n",
    "                        plt.imshow(cv2.cvtColor(annotated_full_image, cv2.COLOR_BGR2RGB))\n",
    "                        plt.title(\"Licensed Plate Detection\")\n",
    "                        plt.axis('off')\n",
    "                        \n",
    "                        # 2. Cropped license plate\n",
    "                        plt.subplot(1, 4, 2)\n",
    "                        plt.imshow(cv2.cvtColor(plate_image, cv2.COLOR_BGR2RGB))\n",
    "                        plt.title(\"Cropped License Plate\")\n",
    "                        plt.axis('off')\n",
    "                        \n",
    "                        # 3. Image preprocessed (binarized)\n",
    "                        plt.subplot(1, 4, 3)\n",
    "                        plt.imshow(preprocessed_plate, cmap='gray')\n",
    "                        plt.title(\"Image Preprocessed\")\n",
    "                        plt.axis('off')\n",
    "                        \n",
    "                        # 4. Characters segmented\n",
    "                        plt.subplot(1, 4, 4)\n",
    "                        plt.imshow(cv2.cvtColor(output_image, cv2.COLOR_BGR2RGB))\n",
    "                        plt.title(f\"characters Segmented: {len(characters)}\")\n",
    "                        plt.axis('off')\n",
    "                        \n",
    "                        plt.suptitle(f\"Image {i+1}\")\n",
    "                        plt.tight_layout()\n",
    "                        plt.show()\n",
    "\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"Could not crop license plate. Skipping.\")\n",
    "            else:\n",
    "                print(f\"\\n--- Processing Image {i+1} ---\")\n",
    "                print(\"No license plate detected in this image.\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # OCR Functions \n",
    "    # =========================================================================\n",
    "        \n",
    "    def load_char_recognizer(self, model_path=\"char_recognizer.pt\"):\n",
    "        self.char_model = CharRecognizer(len(CHAR_CLASSES)).to(self.device)\n",
    "        self.char_model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "        self.char_model.eval()\n",
    "\n",
    "    def ocr_characters(self, characters):\n",
    "        \"\"\"\n",
    "        OCR using a simple CNN model for character recognition\n",
    "        \"\"\"\n",
    "        plate_text = \"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((28,28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "\n",
    "        for char_info in characters:\n",
    "            img_char = char_info['image']\n",
    "            pil_img = cv2.cvtColor(img_char, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = transforms.ToPILImage()(pil_img)\n",
    "            img_tensor = transform(pil_img).unsqueeze(0).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.char_model(img_tensor)\n",
    "                pred_idx = output.argmax(1).item()\n",
    "                plate_text += CHAR_CLASSES[pred_idx]\n",
    "\n",
    "        return plate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6219e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adria\\AppData\\Local\\Temp\\ipykernel_2076\\1165006771.py:217: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.char_model.load_state_dict(torch.load(model_path, map_location=self.device))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'char_recognizer.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m detector \u001b[38;5;241m=\u001b[39m LicensePlateDetector()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# ! LOAD THE OCR MODEL\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_char_recognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchar_recognizer.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# best_model_path = detector.train_model(dataset_path, epochs=10, batch=8)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124madria\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive - UAB\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4 ENGINY\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mProcessament Imatge i Video\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRepte Matriculas\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLicense-Plate-Detection\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbest_license_plate.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[40], line 217\u001b[0m, in \u001b[0;36mLicensePlateDetector.load_char_recognizer\u001b[1;34m(self, model_path)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_char_recognizer\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchar_recognizer.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_model \u001b[38;5;241m=\u001b[39m CharRecognizer(\u001b[38;5;28mlen\u001b[39m(CHAR_CLASSES))\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\adria\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1317\u001b[0m     pickle_load_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1319\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1321\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1322\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1323\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\adria\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:659\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    658\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    661\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\adria\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\serialization.py:640\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 640\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'char_recognizer.pt'"
     ]
    }
   ],
   "source": [
    "#* ================= MAIN EXECUTION =================\n",
    "\n",
    "# dataset_path = \"/content/BD/BD_LicensePlate\"\n",
    "# test_dir = \"/content/BD/BD_LicensePlate/images/test\"\n",
    "# output_dir = \"/content/test_results\"\n",
    "\n",
    "dataset_path = r\"C:\\Users\\adria\\OneDrive - UAB\\4 ENGINY\\Processament Imatge i Video\\Repte Matriculas\\BD_LicensePlate\"\n",
    "test_dir = r\"C:\\Users\\adria\\OneDrive - UAB\\4 ENGINY\\Processament Imatge i Video\\Repte Matriculas\\BD_LicensePlate\\images\\test\"\n",
    "output_dir = r\"C:\\Users\\adria\\OneDrive - UAB\\4 ENGINY\\Processament Imatge i Video\\Repte Matriculas\\test_results\"\n",
    "\n",
    "detector = LicensePlateDetector()\n",
    "# ! LOAD THE OCR MODEL\n",
    "detector.load_char_recognizer(model_path=\"char_recognizer.pt\")\n",
    "\n",
    "# best_model_path = detector.train_model(dataset_path, epochs=10, batch=8)\n",
    "best_model_path = r\"C:\\Users\\adria\\OneDrive - UAB\\4 ENGINY\\Processament Imatge i Video\\Repte Matriculas\\License-Plate-Detection\\models\\best_license_plate.pt\"\n",
    "# best_model_path = r\"C:\\Users\\adria\\OneDrive - UAB\\4 ENGINY\\Processament Imatge i Video\\Repte Matriculas\\License-Plate-Detection\\models\\best.pt\"\n",
    "prediction_results = detector.detect_license_plate(best_model_path, test_dir, output_dir, conf=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0eedd5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Image 1 ---\n",
      "Detected License Plate 1: Bbox (1510, 762, 1879, 1063)\n",
      "Found 5 character candidates.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LicensePlateDetector' object has no attribute 'char_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Segmentation and display of characters\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_and_show_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction_results\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 162\u001b[0m, in \u001b[0;36mLicensePlateDetector.process_and_show_results\u001b[1;34m(self, prediction_results)\u001b[0m\n\u001b[0;32m    159\u001b[0m characters, preprocessed_plate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_characters(plate_image)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(characters)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m character candidates.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 162\u001b[0m plate_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mocr_characters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcharacters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOCR Result, Detected Plate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplate_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;66;03m# Create output image with bounding boxes\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 239\u001b[0m, in \u001b[0;36mLicensePlateDetector.ocr_characters\u001b[1;34m(self, characters)\u001b[0m\n\u001b[0;32m    236\u001b[0m img_tensor \u001b[38;5;241m=\u001b[39m transform(pil_img)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 239\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchar_model\u001b[49m(img_tensor)\n\u001b[0;32m    240\u001b[0m     pred_idx \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    241\u001b[0m     plate_text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m CHAR_CLASSES[pred_idx]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LicensePlateDetector' object has no attribute 'char_model'"
     ]
    }
   ],
   "source": [
    "# Segmentation and display of characters\n",
    "\n",
    "detector.process_and_show_results(prediction_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
